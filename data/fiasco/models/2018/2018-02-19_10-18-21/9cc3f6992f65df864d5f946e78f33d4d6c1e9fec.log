"2018-02-19 10:18:21 +0100"
diff --git a/src/Kconfig b/src/Kconfig
index c595cfd..5f040f2 100644
--- a/src/Kconfig
+++ b/src/Kconfig
@@ -344,6 +344,20 @@ config CPU_LOCAL_MAP
 	  
 	  If unsure say N.
 
+config INTEL_IA32_BRANCH_BARRIERS
+	bool "Enable strict prediction berriers (IBRS,IBPB,STIBP)" if AMD64 && KERNEL_ISOLATION
+	depends on AMD64 && KERNEL_ISOLATION
+	help
+	  Use indirect branch prediction barriers to prevent
+	  speculation-based cache timing side-channels.
+	  This option enables Intel specific indirect branch control
+	  mitigation:
+	    (a) IBRS on all kernel entries to prevent in-kernel
+	        prediction attacks.
+	    (b) STIBP to prevent cross hyper-thred attacks.
+	    (c) IBPB after address space switches to prevent cross
+	        application attacks.
+
 config NO_IO_PAGEFAULT
 	bool "Disable IO-Port fault IPC" if (IA32 || AMD64) && !KERNEL_ISOLATION
 	depends on IA32 || AMD64
diff --git a/src/kern/ia32/64/cpu-64.cpp b/src/kern/ia32/64/cpu-64.cpp
index ad7fb0b..4a65a7f 100644
--- a/src/kern/ia32/64/cpu-64.cpp
+++ b/src/kern/ia32/64/cpu-64.cpp
@@ -45,7 +45,10 @@ PUBLIC
 void
 Cpu::set_fast_entry(void (*func)())
 {
-  *reinterpret_cast<Signed32 *>(Mem_layout::Mem_layout::Kentry_cpu_page + 0xc5) = (Signed32)(Signed64)func;
+  extern char const syscall_entry_code[];
+  extern char const syscall_entry_reloc[];
+  auto ofs = syscall_entry_reloc - syscall_entry_code + 3; // 3 byte movebas
+  *reinterpret_cast<Signed32 *>(Mem_layout::Mem_layout::Kentry_cpu_page + ofs + 0xa0) = (Signed32)(Signed64)func;
 }
 
 PUBLIC inline
diff --git a/src/kern/ia32/64/entry-native.S b/src/kern/ia32/64/entry-native.S
index 76ea26a..bfca673 100644
--- a/src/kern/ia32/64/entry-native.S
+++ b/src/kern/ia32/64/entry-native.S
@@ -60,6 +60,7 @@ entry_vec08_dbf:
 	push	%r13
 	push	%r14
 	push	%r15
+	IA32_IBRS_CLOBBER
 	mov	%rsp,%rdi		// 1st arg: trap state
 	call	thread_handle_double_fault
 	jmp	entry_vec08_dbf
@@ -220,6 +221,9 @@ syscall_entry_code:
 	pushq	$(GDT_DATA_USER | 3)
 	pushq	CPUE_STACK(CPUE_OFFSET + 0, %rip)
 	push	%r11				/* save user rflags */
+	IA32_IBRS
+	.global syscall_entry_reloc
+syscall_entry_reloc:
 	mov	$entry_sys_fast_ipc_c, %r11
 	jmp	*%r11
 	.global syscall_entry_code_end
@@ -244,6 +248,16 @@ safe_iret:
 
 	mov	$0xffff817fffffc000, %r14
 	mov	%r13, CPUE_STACK( 0, %r14)
+#ifdef CONFIG_INTEL_IA32_BRANCH_BARRIERS
+	/* check for exit flags (need IBTPB?) */
+	mov	CPUE_EXIT(%r14), %r13
+	test	$(CPUE_EXIT_NEED_IBPB), %r13
+	jz	22f
+	and	$(~CPUE_EXIT_NEED_IBPB), %r13
+	mov	%r13, CPUE_EXIT(%r14)
+	IA32_IBPB
+22:
+#endif
 	mov	(%rsp), %r13
 	mov	%r13, CPUE_STACK( 8, %r14)
 	mov	32(%rsp), %r13
diff --git a/src/kern/ia32/64/entry.S b/src/kern/ia32/64/entry.S
index 20155bd..d3b3bcd 100644
--- a/src/kern/ia32/64/entry.S
+++ b/src/kern/ia32/64/entry.S
@@ -101,6 +101,7 @@ EXCEPTION	0x13, vec13_simd_err
 	   from outside. */
 _slowtraps:
 slowtraps:
+	IA32_IBRS
 	mov	%rsp,%rdi		/* ARG1: address of trap_state */
 	mov	$0, %esi		/* ARG2: default CPU = 0 */
 	cld
@@ -160,6 +161,7 @@ entry_vec0e_page_fault:
 	cld
 	SWITCH_TO_KERNEL_CR3	1
 	SAVE_SCRATCH
+	IA32_IBRS
 
 /* We must reset the cancel flag here atomically
    if we are entering fresh from user mode and an IPC might occur.
@@ -256,6 +258,7 @@ GATE_ENTRY	APIC_IRQ_BASE, entry_int_timer, (ACC_PL_K | ACC_INTR_GATE)
 entry_int_timer:
 	SWITCH_TO_KERNEL_CR3	0
 	SAVE_SCRATCH
+	IA32_IBRS
 do_timer_interrupt:
 	cld
 	mov	SCRATCH_REGISTER_SIZE(%rsp),%rdi /* pass rip for logging */
@@ -270,6 +273,7 @@ entry_int_timer_slow:
 	cld
 	SWITCH_TO_KERNEL_CR3	0
 	SAVE_SCRATCH
+	IA32_IBRS
 	call	thread_timer_interrupt_slow	/* enter with disabled irqs */
 in_timer_interrupt_slow:
 	jmp	do_timer_interrupt
@@ -333,6 +337,7 @@ __generic_irq_entry:
 	.p2align 4
 	.type	all_irqs,@function
 all_irqs:
+	IA32_IBRS
 	cld
 	call	irq_interrupt			/* enter with disabled irqs */
 in_interrupt:
@@ -373,6 +378,7 @@ entry_\name:
 all_syscalls:
 	cld
 	SAVE_STATE
+	IA32_IBRS
 	ESP_TO_TCB_AT %rbx
 	testl	$VAL__Thread_alien_or_vcpu_user, OFS__THREAD__STATE (%rbx)
 	jnz	alien_sys_call
@@ -397,6 +403,7 @@ entry_sys_ipc_c:
 	sub	$16, %rsp
 	push	%rax
 	SAVE_STATE
+	IA32_IBRS_CLOBBER
 	ESP_TO_TCB_AT %rbx
 	testl	$VAL__Thread_alien_or_vcpu_user, OFS__THREAD__STATE (%rbx)
 	jnz	alien_sys_ipc_c
diff --git a/src/kern/ia32/64/low_level.h b/src/kern/ia32/64/low_level.h
index c4b1abd..9577382 100644
--- a/src/kern/ia32/64/low_level.h
+++ b/src/kern/ia32/64/low_level.h
@@ -14,6 +14,43 @@
 #define CPUE_KSP_OFS 8
 #define CPUE_KSP(reg) 8(reg)
 #define CPUE_CR3(reg) 0(reg)
+#define CPUE_EXIT(reg) 16(reg)
+#define CPUE_EXIT_NEED_IBPB 1
+
+#if defined(CONFIG_KERNEL_ISOLATION) && defined(CONFIG_INTEL_IA32_BRANCH_BARRIERS)
+.macro IA32_IBRS_CLOBBER
+	mov $0x48, %ecx
+	mov $0, %edx
+	mov $3, %eax
+	wrmsr
+.endm
+.macro IA32_IBRS
+	pushq %rax
+	pushq %rcx
+	pushq %rdx
+	IA32_IBRS_CLOBBER
+	popq %rdx
+	popq %rcx
+	popq %rax
+.endm
+.macro IA32_IBPB
+	pushq %rax
+	pushq %rcx
+	pushq %rdx
+	mov	$0x49, %ecx
+	xor	%edx, %edx
+	xor	%eax, %eax
+	wrmsr
+	popq %rdx
+	popq %rcx
+	popq %rax
+.endm
+#else
+.macro IA32_IBRS_CLOBBER
+.endm
+.macro IA32_IBRS
+.endm
+#endif
 
 .macro SAFE_SYSRET
 	/* make RIP canonical, workaround for intel IA32e flaw */
@@ -21,6 +58,15 @@
 	sar     $16, %rcx
 #ifdef CONFIG_KERNEL_ISOLATION
 	mov	$0xffff817fffffc000, %r15
+#ifdef CONFIG_INTEL_IA32_BRANCH_BARRIERS
+	mov	CPUE_EXIT(%r15), %r11
+	test	$(CPUE_EXIT_NEED_IBPB), %r11
+	jz	333f
+	and	$(~CPUE_EXIT_NEED_IBPB), %r11
+	mov	%r11, CPUE_EXIT(%r15)
+	IA32_IBPB
+333:
+#endif
 	mov	CPUE_CR3(%r15), %r15
 	or	$0x1000, %r15
 #endif
diff --git a/src/kern/ia32/64/thread-ia32-64.cpp b/src/kern/ia32/64/thread-ia32-64.cpp
index 00484d3..00b2445 100644
--- a/src/kern/ia32/64/thread-ia32-64.cpp
+++ b/src/kern/ia32/64/thread-ia32-64.cpp
@@ -32,12 +32,22 @@ Thread::fast_return_to_user(Mword ip, Mword sp, T arg)
   assert(cpu_lock.test());
   assert(current() == this);
 
+  Address *p = (Address *)Mem_layout::Kentry_cpu_page;
+
+#ifdef CONFIG_INTEL_IA32_BRANCH_BARRIERS
+  if (p[2] & 1)
+    {
+      p[2] &= ~1UL;
+      Cpu::wrmsr(0, 0, 0x49);
+    }
+#endif
+
   asm volatile
     ("mov %[sp], %%rsp \t\n"
      "mov %[flags], %%r11 \t\n"
      "jmp safe_sysret \t\n"
      :
-     : [cr3] "a" (*((Address *)Mem_layout::Kentry_cpu_page) | 0x1000),
+     : [cr3] "a" (p[0] | 0x1000),
        [flags] "i" (EFLAGS_IF), "c" (ip), [sp] "r" (sp), "D"(arg)
     );
   __builtin_trap();
diff --git a/src/kern/ia32/cpu-ia32.cpp b/src/kern/ia32/cpu-ia32.cpp
index eb0c3d0..a85955d 100644
--- a/src/kern/ia32/cpu-ia32.cpp
+++ b/src/kern/ia32/cpu-ia32.cpp
@@ -991,6 +991,8 @@ Cpu::identify()
 
     _vendor = (Cpu::Vendor)i;
 
+    init_indirect_branch_mitigation();
+
     switch (max)
       {
       default:
@@ -1314,6 +1316,8 @@ Cpu::pm_resume()
 
       set_tss();
     }
+  init_indirect_branch_mitigation();
+
   init_sysenter();
   wrmsr(_suspend_tsc, MSR_TSC);
 
@@ -1825,3 +1829,38 @@ PUBLIC static inline
 void
 Cpu::set_gs(Unsigned16 val)
 { asm volatile ("mov %0, %%gs" : : "rm" (val)); }
+
+
+//----------------------------------------------------------------------------
+IMPLEMENTATION[(ia32 || amd64 || ux) && !intel_ia32_branch_barriers]:
+
+PRIVATE inline FIASCO_INIT_CPU_AND_PM
+void
+Cpu::init_indirect_branch_mitigation()
+{}
+
+//----------------------------------------------------------------------------
+IMPLEMENTATION[(ia32 || amd64) && intel_ia32_branch_barriers]:
+
+PRIVATE FIASCO_INIT_CPU_AND_PM
+void
+Cpu::init_indirect_branch_mitigation()
+{
+  if (_vendor == Vendor_intel)
+    {
+      Unsigned32 a, b, c, d;
+      cpuid(0, &a, &b, &c, &d);
+      if (a < 7)
+        panic("intel CPU does not support IBRS, IBPB, STIBP (cpuid max < 7)\n");
+
+      cpuid(7, 0, &a, &b, &c, &d);
+      if (!(d & (1UL << 26)))
+        panic("IBRS / IBPB not supported by CPU: %x\n", d);
+
+      if (!(d & (1UL << 27)))
+        panic("STIBP not supported by CPU: %x\n", d);
+
+      // enable STIBP
+      wrmsr(2, 0x48);
+    }
+}
diff --git a/src/kern/ia32/kmem-ia32.cpp b/src/kern/ia32/kmem-ia32.cpp
index 9686934..fd84142 100644
--- a/src/kern/ia32/kmem-ia32.cpp
+++ b/src/kern/ia32/kmem-ia32.cpp
@@ -884,7 +884,10 @@ Kmem::init_cpu(Cpu &cpu)
   Cpu::set_pdbr(cpu_dir_pa);
 
   cxx::Simple_alloc cpu_m(Kentry_cpu_page, Config::PAGE_SIZE);
-  // CPU dir pa and 
+  // [0] = CPU dir pa
+  // [1] = KSP
+  // [2] = EXIT flags
+  // [3] = reserved
   write_now(cpu_m.alloc<Mword>(4), cpu_dir_pa);
   setup_cpu_structures_isolation(cpu, cpu_dir, &cpu_m);
 
diff --git a/src/kern/ia32/mem_space-ia32.cpp b/src/kern/ia32/mem_space-ia32.cpp
index 04a6ce9..a51f8df 100644
--- a/src/kern/ia32/mem_space-ia32.cpp
+++ b/src/kern/ia32/mem_space-ia32.cpp
@@ -473,6 +473,12 @@ Mem_space::make_current()
   asm volatile ("" : : : "memory");
   Address pd_pa = access_once(reinterpret_cast<Address *>(Mem_layout::Kentry_cpu_page));
   Cpu::set_pdbr(pd_pa);
+#else
+#ifdef CONFIG_INTEL_IA32_BRANCH_BARRIERS
+  Address *ca = reinterpret_cast<Address *>(Mem_layout::Kentry_cpu_page);
+  // set EXIT flags NEEDS IBPB
+  ca[2] |= 1;
+#endif
 #endif
   _current.cpu(current_cpu()) = this;
 }